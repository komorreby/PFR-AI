# План по внедрению Retrieval-Augmented Generation (RAG) для автоматической проверки заявлений на пенсию

## 1. Описание задачи
Пользователь заполняет форму для подачи заявления на пенсию. Программа должна автоматически анализировать введённые данные, сверять их с требованиями законодательства и, если есть несоответствия, выдавать причины отказа и рекомендации по их устранению с указанием конкретных статей закона.

---

## 2. Почему выбран RAG
- **Не требует размеченного датасета** — достаточно иметь тексты законов и постановлений.
- **Легко обновлять** — при изменении законодательства достаточно добавить новые документы.
- **Ответы строго по вашим данным** — система не "галлюцинирует" и не придумывает ответы вне рамок ваших законов.
- **Гибкость** — можно использовать любую LLM (например, Ollama) и любую векторную базу данных.

---

## 3. Архитектура решения

### 3.1. Сбор данных
- Пользователь заполняет форму (React, Chakra UI и т.д.).
- Все данные структурированы: ФИО, дата рождения, стаж, документы, льготы и т.д.

### 3.2. Индексация законов
- Все законы и нормативные документы разбиваются на фрагменты (статьи, пункты, абзацы).
- Для каждого фрагмента строится эмбеддинг (векторное представление смысла) с помощью модели эмбеддингов (например, sbert, bge, Ollama embeddings).
- Все эмбеддинги и исходные тексты сохраняются во векторную базу данных (FAISS, Qdrant, Chroma и др.).
- Используйте фреймворки: LlamaIndex, Haystack, LangChain для автоматизации пайплайна.

### 3.3. Проверка заявления
- На основе заполненной формы формируется описание кейса.
- Описание кейса превращается в эмбеддинг.
- Поиск по векторной базе возвращает N наиболее релевантных фрагментов закона.
- Формируется prompt для LLM: в него подставляются найденные фрагменты и описание кейса.
- В инструкции к LLM: "Отвечай только на основе приведённых ниже фрагментов закона. Если ответа нет — скажи, что информации недостаточно."
- LLM (например, Ollama) генерирует объяснение причины отказа и рекомендации.

### 3.4. Выдача результата
- Пользователь получает список причин отказа и рекомендации с указанием статей закона.
- Всё работает локально, данные не покидают сервер.

---

## 4. Пример пайплайна
1. Загрузка и разметка законов
2. Генерация эмбеддингов
3. Индексация во векторную БД
4. На входе — данные формы
5. Semantic Search по базе законов
6. Формирование prompt
7. Запрос к LLM (Ollama)
8. Ответ пользователю

---

## 5. Инструменты
- **Векторная БД**: FAISS, Qdrant, Chroma
- **Эмбеддинги**: SentenceTransformers (sbert), Ollama embeddings, BGE и др.
- **LLM**: Ollama (Llama-2, Mistral, Qwen и др.)
- **Фреймворки**: LlamaIndex, Haystack, LangChain
- **Язык**: Python

---

## 6. Преимущества RAG
- Нет необходимости в ручной разметке данных
- Легко поддерживать и расширять
- Актуальные и проверяемые ответы
- Безопасность и приватность данных

---

## 7. Рекомендации
- Используйте LlamaIndex для автоматизации разбиения, индексации и поиска по документам.
- Регулярно обновляйте базу законов при изменении законодательства.
- В prompt к LLM всегда явно указывайте: "Отвечай только на основе приведённых фрагментов".
- Для типовых ошибок можно оставить rule-based или ML-классификатор.

---

## 8. Итог
RAG — оптимальный выбор для автоматизации проверки заявлений на пенсию по законам РФ: система выдаёт объяснения и рекомендации строго по вашим данным, не требует размеченного датасета и легко поддерживается.

---

# Этапы реализации RAG для автоматической проверки заявлений на пенсию

## 1. Подготовка документов
- Соберите все законы, постановления, инструкции в электронном виде (TXT, PDF, DOCX).
- Преобразуйте PDF/DOCX в текст (например, с помощью `pdfplumber`, `python-docx`).
- Организуйте документы по папкам или метаданным (название, дата, тип документа).

## 2. Разбиение на фрагменты (чанкование)
- Разбейте каждый документ на смысловые куски: статьи, пункты, абзацы или чанки по 500–1000 символов.
- Присвойте каждому фрагменту уникальный идентификатор и метаданные (откуда взят, номер статьи и т.д.).

## 3. Генерация эмбеддингов
- Используйте модель эмбеддингов (`sentence-transformers`, `bge`, Ollama embeddings) для преобразования каждого фрагмента в вектор.
- Сохраните соответствие "фрагмент — эмбеддинг — метаданные".

## 4. Индексация во векторную базу данных
- Выберите векторную БД (FAISS, Qdrant, Chroma).
- Загрузите эмбеддинги и тексты фрагментов в БД.
- Проверьте, что поиск по базе возвращает релевантные фрагменты по смыслу.

## 5. Интеграция с LLM (например, Ollama)
- Настройте Ollama или другую LLM локально.
- Реализуйте функцию формирования prompt: в него подставляются описание кейса и найденные релевантные фрагменты закона.
- В инструкции к LLM укажите: "Отвечай только на основе приведённых фрагментов. Если ответа нет — скажи, что информации недостаточно."

## 6. Обработка пользовательских заявлений
- Пользователь заполняет форму на сайте.
- Backend формирует описание кейса и получает эмбеддинг запроса.
- Semantic Search по векторной БД возвращает N релевантных фрагментов закона.
- Формируется prompt для LLM, отправляется запрос, получается объяснение и рекомендации.

## 7. Выдача результата пользователю
- На фронтенде отображается список причин отказа и рекомендации с указанием статей закона.
- Можно добавить ссылки на полные тексты законов и инструкции по исправлению ошибок.

## 8. Обновление базы
- При появлении новых законов или изменений — повторить этапы 1–4 (можно автоматизировать скриптом).
- Регулярно тестировать релевантность поиска и корректность объяснений.

---

## Примечания
- Для автоматизации пайплайна используйте LlamaIndex, Haystack или LangChain.
- Для больших объёмов данных и сложных кейсов используйте фильтрацию и ранжирование фрагментов по метаданным.
- Все этапы можно реализовать на Python с помощью готовых библиотек.

---

# Подробная инструкция по полной автоматизации чанкования и пайплайна RAG

## 1. Автоматизация разбиения на фрагменты (чанкование)

### 1.1. Установка необходимых библиотек
```bash
pip install pdfplumber python-docx tqdm
```

### 1.2. Пример скрипта для автоматического чанкования
```python
import os
import pdfplumber
from docx import Document
from tqdm import tqdm

def extract_text_from_pdf(pdf_path):
    text = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text.append(page.extract_text())
    return '\n'.join(text)

def extract_text_from_docx(docx_path):
    doc = Document(docx_path)
    return '\n'.join([para.text for para in doc.paragraphs])

def split_into_chunks(text, chunk_size=800, overlap=100):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = ' '.join(words[i:i+chunk_size])
        if chunk.strip():
            chunks.append(chunk)
    return chunks

def process_documents(input_dir, output_path):
    all_chunks = []
    for filename in tqdm(os.listdir(input_dir)):
        file_path = os.path.join(input_dir, filename)
        if filename.lower().endswith('.pdf'):
            text = extract_text_from_pdf(file_path)
        elif filename.lower().endswith('.docx'):
            text = extract_text_from_docx(file_path)
        elif filename.lower().endswith('.txt'):
            with open(file_path, encoding='utf-8') as f:
                text = f.read()
        else:
            continue

        chunks = split_into_chunks(text)
        for idx, chunk in enumerate(chunks):
            all_chunks.append({
                "doc_name": filename,
                "chunk_id": f"{filename}_chunk_{idx}",
                "text": chunk
            })

    # Сохраняем фрагменты в JSONL
    import json
    with open(output_path, 'w', encoding='utf-8') as f:
        for chunk in all_chunks:
            f.write(json.dumps(chunk, ensure_ascii=False) + '\n')

# Использование:
# process_documents('laws_folder', 'all_chunks.jsonl')
```

---

## 2. Автоматизация генерации эмбеддингов
```python
from sentence_transformers import SentenceTransformer
import json

model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
chunks = []
with open('all_chunks.jsonl', encoding='utf-8') as f:
    for line in f:
        chunks.append(json.loads(line))

texts = [chunk['text'] for chunk in chunks]
embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)

# Сохраняйте эмбеддинги и метаданные для загрузки в векторную БД
```

---

## 3. Автоматизация загрузки эмбеддингов в векторную БД
- Для FAISS, Qdrant, Chroma — используйте их Python API для загрузки эмбеддингов и метаданных.
- Можно объединить предыдущие шаги в один скрипт или пайплайн.

---

## 4. Автоматизация всего пайплайна через LlamaIndex

```python
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex

documents = SimpleDirectoryReader('laws_folder').load_data()
index = VectorStoreIndex.from_documents(documents)
index.storage_context.persist('my_index')
```
- LlamaIndex автоматически обрабатывает все форматы, разбивает на чанки, получает эмбеддинги и индексирует их.
- Для обновления базы просто добавьте новые документы и перезапустите индексацию.

---

## 5. Автоматизация обновления
- Запускайте пайплайн (скрипт или LlamaIndex) по расписанию или при добавлении новых документов.
- Вся обработка и обновление базы происходит автоматически без ручного вмешательства.

---

## 6. Итог
- Чанкование, генерация эмбеддингов, индексация и обновление базы могут быть полностью автоматизированы.
- После настройки пайплайна добавление новых документов занимает минуты и не требует ручной работы.
