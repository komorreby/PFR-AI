# PFR-AI: Бэкенд Анализа Пенсионных Дел

[![Python Version](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Framework](https://img.shields.io/badge/framework-FastAPI-green.svg)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE) <!-- Замените на вашу лицензию, если она есть -->
<!-- Добавьте другие значки, если применимо (статус сборки, покрытие тестами и т.д.) -->

Серверная часть приложения для интеллектуального анализа данных пенсионных дел, построенная на FastAPI.

## Основные Возможности

*   **Прием данных:** Обработка данных пенсионного дела, переданных в формате JSON (структура определяется Pydantic-моделями).
*   **Валидация:** Проверка корректности и полноты входных данных с использованием Pydantic.
*   **Классификация Ошибок:** Запуск модели машинного обучения (scikit-learn/joblib) для выявления распространенных ошибок и несоответствий в данных дела (например, недостаточный стаж/баллы, противоречия в датах, отсутствие документов).
*   **RAG-Анализ Законодательства:** Предоставление системы Retrieval-Augmented Generation (RAG) на базе LlamaIndex для анализа описания дела на соответствие нормам пенсионного законодательства РФ (ФЗ-400, ФЗ-166).
    *   Использует локально запущенный Ollama для LLM (например, `qwen3:latest` - актуальную модель см. в `backend/app/rag_core/config.py`) и локальные модели для эмбеддингов (`jina-embeddings-v3`) и реранжирования (`cross-encoder-russian-msmarco`).
    *   Автоматически индексирует PDF-документы законов из папки `data/` при первом запуске.
    *   Применяет продвинутые техники: иерархический парсинг документов, фильтрацию по метаданным (тип пенсии, статья закона).
*   **Хранение Истории:** Сохранение информации об обработанных делах и результатах анализа в базе данных SQLite (с использованием SQLAlchemy в асинхронном режиме).
*   **Генерация Отчетов:** Возможность генерации документов (PDF/DOCX) с базовыми результатами анализа ошибок для конкретного дела.
*   **(Опционально/Отдельно) OCR Документов:** Включает скрипт `documentOCR.py` для распознавания текста из сканов/фотографий документов (паспорт, СНИЛС, трудовая) с использованием Tesseract и EasyOCR, и извлечения ключевой информации. *Примечание: На данный момент этот скрипт не интегрирован напрямую в API эндпоинты.*

## API Эндпоинты

Подробную интерактивную документацию (Swagger UI) можно найти по адресу `/docs` после запуска сервера.

*   **`GET /`**
    *   **Описание:** Проверка доступности сервера.
    *   **Ответ (200 OK):** `{"message": "PFR-AI Backend is running!"}`

*   **`POST /process`**
    *   **Описание:** Основной эндпоинт для комплексного анализа пенсионного дела. Выполняет валидацию данных, запускает классификатор ошибок, формирует объяснение с использованием RAG-анализа и сохраняет дело и результат в БД.
    *   **Тело запроса:** JSON объект, соответствующий Pydantic модели `CaseDataInput` (см. `app/models.py`).
    *   **Ответ (200 OK):** JSON объект, соответствующий Pydantic модели `ProcessOutput`, содержащий поля `status`, `explanation` и `errors`.
    *   **Ответ (422 Unprocessable Entity):** Ошибка валидации входных данных.

*   **`POST /api/v1/analyze_case`**
    *   **Описание:** Эндпоинт для выполнения RAG-анализа на основе структурированных данных о деле.
    *   **Тело запроса:** JSON объект, соответствующий Pydantic модели `CaseDataInput` (см. `app/models.py`). Внутренне данные форматируются для передачи в RAG-систему.
    *   **Ответ (200 OK):** JSON объект, соответствующий Pydantic модели `CaseAnalysisResponse`, содержащий `analysis_result` (текстовый результат анализа от RAG) и `confidence_score`.
    *   **Ответ (503 Service Unavailable):** Если RAG-движок не инициализирован.
    *   **Ответ (500 Internal Server Error):** Внутренняя ошибка при выполнении RAG-запроса.

*   **`GET /history`**
    *   **Описание:** Получение списка последних обработанных дел из базы данных.
    *   **Query Параметры:**
        *   `skip` (int, опционально, default=0): Смещение для пагинации.
        *   `limit` (int, опционально, default=100): Максимальное количество записей.
    *   **Ответ (200 OK):** Список JSON объектов, соответствующих Pydantic модели `CaseHistoryEntry`.

*   **`GET /download_document/{case_id}`**
    *   **Описание:** Скачивание сгенерированного отчета (PDF или DOCX) с базовыми ошибками для указанного дела.
    *   **Path Параметры:**
        *   `case_id` (int): ID дела из истории.
    *   **Query Параметры:**
        *   `format` (str, опционально, default='pdf'): Формат документа ('pdf' или 'docx').
    *   **Ответ (200 OK):** Файл отчета (`application/pdf` или `application/vnd.openxmlformats-officedocument.wordprocessingml.document`).
    *   **Ответ (404 Not Found):** Если дело с указанным `case_id` не найдено.
    *   **Ответ (500 Internal Server Error):** Ошибка при генерации документа.

### Распознавание данных документов по фото (`/api/v1/extract_document_data`)

Этот эндпоинт позволяет загрузить изображение документа (паспорт РФ или СНИЛС) и получить извлеченные данные в формате JSON.

*   **Метод:** `POST`
*   **URL:** `/api/v1/extract_document_data`
*   **Тело запроса (multipart/form-data):**
    *   `document_type`: Строка, тип документа. Допустимые значения: "passport", "snils".
    *   `image`: Файл изображения документа.
*   **Ответ успешный (200 OK):** JSON с данными документа (см. модели `PassportData` или `SnilsData` в `app/models.py` в зависимости от `document_type`).
*   **Ответ с ошибкой:**
    *   `400 Bad Request`: Если загружен неверный тип файла или не указан/неверный `document_type`.
    *   `500 Internal Server Error`: Если произошла ошибка при обработке изображения или извлечении данных.

**Пример использования (curl для паспорта):**
```bash
curl -X POST -F "document_type=passport" -F "image=@/путь/к/вашему/паспорту.jpg" http://localhost:8000/api/v1/extract_document_data
```

**Пример использования (curl для СНИЛС):**
```bash
curl -X POST -F "document_type=snils" -F "image=@/путь/к/вашему/снилс.jpg" http://localhost:8000/api/v1/extract_document_data
```

#### Ответ для `OTHER` (другой документ):

Возвращает JSON объект со следующими полями в ключе `data`:

*   `identified_document_type: Optional[str]`: Тип документа, как его определила мультимодальная модель (например, "Свидетельство о рождении", "Договор").
*   `standardized_document_type: Optional[str]`: Стандартизированный тип документа после сверки с внутренним списком известных пенсионных документов (например, "Свидетельство о рождении ребенка"). Если тип, определенный моделью, не найден в списке или недостаточно похож, это поле может быть `null` или повторять значение `identified_document_type`.
*   `extracted_fields: Optional[Dict[str, Any]]`: Словарь с извлеченными полями и их значениями (ключ-значение).
*   `multimodal_assessment: Optional[str]`: Оценка документа мультимодальной моделью (качество, читаемость, полнота и т.д.).
*   `text_llm_reasoning: Optional[str]`: Дополнительный анализ или "осмысление" от текстовой LLM на основе данных от мультимодальной модели и описания документа.

Пример ответа для `OTHER`:

```json
{
  "document_type": "other",
  "data": {
    "identified_document_type": "Свид-во о рождении ребенка Иванова И.И.",
    "standardized_document_type": "Свидетельство о рождении ребенка",
    "extracted_fields": {
      "фио_ребенка": "Иванов Иван Иванович",
      "дата_рождения": "01.01.2010",
      "место_рождения": "г. Москва",
      "отец_фио": "Иванов Иван Петрович",
      "мать_фио": "Иванова (Сидорова) Мария Сергеевна"
    },
    "multimodal_assessment": "Изображение хорошего качества, текст читаемый, все основные поля присутствуют.",
    "text_llm_reasoning": "Проанализирована информация о документе... Это свидетельство о рождении, выданное на имя Иванова Ивана Ивановича. Основные реквизиты присутствуют и читаемы. Документ может быть использован для подтверждения факта рождения и родственных связей при оформлении пенсии."
  }
}
```

## Установка и Запуск

1.  **Клонирование репозитория:**
    ```bash
    git clone <your-repo-url>
    cd PFR-AI/backend
    ```

2.  **Требования:**
    *   Python 3.9 или выше.
    *   **Ollama:** Установленный и запущенный Ollama (см. [https://ollama.com/](https://ollama.com/)).
    *   **Модели для Ollama:** Убедитесь, что необходимые LLM скачаны (например, `qwen3:latest` - актуальную модель см. в `backend/app/rag_core/config.py`):
        ```bash
        ollama pull qwen3:latest # Пример LLM (см. config.py)
        # ollama pull mxbai-embed-large # Пример, если бы эмбеддинги были через Ollama
        ```
        Ollama должен быть доступен по адресу `http://localhost:11434` (стандартный).
    *   **(Windows)** Tesseract OCR: Если планируется использовать `documentOCR.py`, установите Tesseract (см. [инструкции](https://tesseract-ocr.github.io/tessdoc/Installation.html)) и убедитесь, что путь к `tesseract.exe` и `tessdata` указан верно в `documentOCR.py`.
    *   **(Windows)** Poppler: Если планируется использовать `documentOCR.py` для PDF, установите Poppler (см. [инструкции](https://github.com/oschwartz10612/poppler-windows/releases/)) и укажите путь к папке `bin` в `documentOCR.py`.

3.  **Виртуальное окружение:**
    ```bash
    python -m venv venv
    # Windows
    .\venv\Scripts\activate
    # macOS/Linux
    source ./venv/bin/activate
    ```

4.  **Установка зависимостей:**
    *Примечание: Установка зависимостей, особенно `torch` и связанных с ним библиотек для эмбеддингов/реранкера, может занять время и потребовать значительного дискового пространства.*
    ```bash
    pip install -r requirements.txt
    ```

5.  **(Опционально) Конфигурация:**
    *   Основные параметры RAG (имена моделей, пути) настраиваются в `backend/app/rag_core/config.py`.
    *   Рекомендуется использовать переменные окружения для переопределения настроек (например, `OLLAMA_BASE_URL`). Можно создать файл `.env` в папке `backend/` (поддерживается `python-dotenv`):
      ```dotenv
      # Пример .env файла
      # OLLAMA_BASE_URL=http://другой_хост:11434
      # LOGGING_LEVEL=DEBUG
      ```

6.  **Индексация RAG (при первом запуске):**
    *   При первом запуске сервера будет выполнена индексация PDF-документов из папки `backend/data/`. Это **необходимо** для работы RAG-системы и может занять **значительное время** (от нескольких минут до десятков минут в зависимости от объема документов и мощности системы).
    *   Следите за логами сервера для отслеживания прогресса. Файлы индекса будут сохранены в `backend/data/`.

7.  **Запуск сервера:**
    ```