План по Созданию и Интеграции Графа Знаний (KG)
Фаза I: Проектирование и Подготовка
Цель: Заложить прочный фундамент для графа знаний, определив его структуру и выбрав инструменты.

Этап 1: Определение Онтологии Графа (Schema Definition)

Задача: Создать формальное описание типов информации (сущностей) и их взаимосвязей (отношений), которые будут храниться в графе. Это "скелет" вашего графа знаний.

Ключевые действия:

Шаг 1.1: Анализ Предметной Области и Определение Ключевых Сущностей (Nodes).

Что делать:

Провести глубокий анализ текстов пенсионного законодательства (ФЗ-400, ФЗ-166 и др.), уделяя особое внимание часто встречающимся понятиям, их определениям и роли в процессе назначения пенсии.

Определить пилотный набор пользовательских запросов (например, "условия назначения страховой пенсии по старости", "досрочная пенсия для северян", "какие документы нужны для социальной пенсии по инвалидности").

На основе анализа и запросов составить первоначальный список типов сущностей.

Примеры: Закон, СтатьяЗакона, ПунктСтатьи, УсловиеНазначенияПенсии, ТипПенсии, КатегорияГраждан, Документ, Льгота, КлючевойТермин (например, "страховой стаж", "ИПК", "пенсионный возраст").

Обсуждение/Решение: Какие 3-5 типов сущностей являются абсолютно критичными для пилотного запуска?

Шаг 1.2: Определение Атрибутов для Каждого Типа Сущностей.

Что делать: Для каждого выбранного типа сущности определить набор свойств, которые его характеризуют.

Пример для СтатьяЗакона:

id_статьи (уникальный идентификатор, например, "ФЗ-400_Ст.8")

номер_статьи (текстовый, например, "Статья 8", "8.1")

название_статьи (если есть, например, "Условия назначения страховой пенсии по старости")

текст_статьи_оригинал (полный текст статьи или наиболее репрезентативный фрагмент)

ссылка_на_чанк_в_векторной_БД (если текст статьи разбит на чанки для векторного поиска)

принадлежит_закону (ссылка на сущность Закон)

версия_редакции (если актуально)

Пример для УсловиеНазначенияПенсии:

id_условия (уникальный идентификатор)

описание_условия (например, "Достижение возраста X лет")

значение_условия (например, "65 лет", "15 лет", "30 баллов")

нормативная_ссылка (ссылка на СтатьяЗакона или ПунктСтатьи, где это условие определено)

Обсуждение/Решение: Какие атрибуты обязательны, а какие опциональны для каждого типа сущности на пилотном этапе? Как обеспечить уникальность идентификаторов?

Шаг 1.3: Определение Основных Типов Отношений (Edges).

Что делать: Описать, как определенные типы сущностей могут быть связаны друг с другом. Отношения должны отражать семантические связи в предметной области.

Примеры:

(Закон) -[СОДЕРЖИТ]-> (СтатьяЗакона)

(СтатьяЗакона) -[СОДЕРЖИТ]-> (ПунктСтатьи)

(СтатьяЗакона) -[РЕГУЛИРУЕТ]-> (ТипПенсии)

(СтатьяЗакона) -[УСТАНАВЛИВАЕТ_УСЛОВИЕ]-> (УсловиеНазначенияПенсии)

(ТипПенсии) -[ТРЕБУЕТ_ДОКУМЕНТ]-> (Документ)

(УсловиеНазначенияПенсии) -[ПРИМЕНИМО_К_КАТЕГОРИИ]-> (КатегорияГраждан)

(СтатьяЗакона_А) -[ССЫЛАЕТСЯ_НА]-> (СтатьяЗакона_Б)

Обсуждение/Решение: Какие 2-3 типа отношений наиболее важны для улучшения поиска и понимания контекста LLM на первом этапе?

Шаг 1.4: (Опционально) Определение Атрибутов для Отношений.

Что делать: В некоторых случаях сами отношения могут иметь атрибуты.

Пример: Отношение (СтатьяЗакона) -[ССЫЛАЕТСЯ_НА {тип_ссылки: "уточнение", дата_ссылки: "..."}]-> (СтатьяЗакона)

Обсуждение/Решение: Нужны ли атрибуты для отношений на пилотном этапе?

Шаг 1.5: Визуализация и Документирование Онтологии.

Что делать:

Создать диаграмму онтологии (например, с помощью draw.io, Lucidchart или специализированных инструментов для моделирования онтологий).

Подготовить документ, описывающий все типы сущностей, их атрибуты (с типами данных и примерами), все типы отношений и их возможные пары (субъект-объект).

Результат: Четко определенная и задокументированная схема графа.

Этап 2: Выбор Технологического Стека и Подготовка Инфраструктуры

Задача: Выбрать инструменты для создания, хранения и взаимодействия с графом знаний.

Ключевые действия:

Шаг 2.1: Выбор Графовой СУБД.

Что делать:

Проанализировать доступные графовые СУБД (Neo4j, Amazon Neptune, ArangoDB, RDF-хранилища типа Apache Jena/Stardog, TigerGraph).

Оценить их по критериям: простота установки и использования, язык запросов (Cypher, Gremlin, SPARQL), производительность, масштабируемость, поддержка сообщества, наличие Python-драйверов, интеграция с LlamaIndex.

Рекомендация для старта: Neo4j часто является хорошим выбором из-за зрелости, удобного языка Cypher и хорошей поддержки.

Обсуждение/Решение: Какая СУБД будет использоваться для пилотного проекта? Будет ли она развернута локально или в облаке?

Шаг 2.2: Установка и Настройка Графовой СУБД.

Что делать: Установить выбранную СУБД (например, Neo4j Desktop для локальной разработки). Освоить базовые команды администрирования и взаимодействия с СУБД.

Результат: Рабочее окружение графовой СУБД.

Шаг 2.3: Выбор Инструментов для Извлечения Сущностей и Отношений (NER & RE).

Что делать:

Для Rule-based NER/RE: Подготовить набор регулярных выражений (на основе существующих в document_parser.py и новых) и словарей ключевых терминов (для условий, типов пенсий и т.д.).

Для ML-based NER/RE (если планируется):

Исследовать готовые русскоязычные модели: Natasha, spaCy (с русской моделью ru_core_news_sm/md/lg), DeepPavlov.

Оценить их качество "из коробки" на примерах ваших текстов.

Рассмотреть возможность использования Hugging Face Transformers для доступа к более широкому спектру моделей.

Для LLM-based NER/RE: Подготовить шаблоны промптов для вашей LLM (Qwen2), ориентированные на извлечение сущностей и триплетов (субъект, предикат, объект) из текстовых фрагментов.

Обсуждение/Решение: Какой подход или комбинация подходов к NER/RE будет использоваться на первом этапе? Какие библиотеки Python будут задействованы (re, Natasha, spaCy, transformers)?

Шаг 2.4: Подготовка Окружения для Разработки Скриптов Извлечения.

Что делать: Настроить Python-окружение с необходимыми библиотеками (драйвер для графовой СУБД, библиотеки для NER/RE).

Результат: Готовность к написанию кода для наполнения графа.

Фаза II: Реализация Извлечения и Построения Графа
Цель: Наполнить граф знаниями, извлеченными из документов.

Этап 3: Извлечение Данных (Information Extraction)

Задача: Автоматически извлечь сущности и отношения из текстовых документов в соответствии с разработанной онтологией.

Ключевые действия:

Шаг 3.1: Адаптация Существующего Парсера Документов.

Что делать:

Модифицировать document_parser.py (или создать новый модуль), чтобы он не только разбивал текст на чанки и извлекал базовую метаинформацию (имя файла, статья из заголовка), но и подготавливал текст для более глубокого NER/RE.

Возможно, потребуется более гранулярное разбиение или сохранение исходной структуры документа для лучшего контекста при извлечении отношений.

Результат: Функция или класс, возвращающий текст и его структурные метаданные, готовые для NER/RE.

Шаг 3.2: Реализация Модуля NER.

Что делать (выбрать один или комбинацию подходов):

Rule-based NER: Написать Python-функции, использующие регулярные выражения и словари для поиска и классификации сущностей (например, "Статья X", "ФЗ-Y", "пенсионный возраст", "страховая пенсия по старости").

ML-based NER: Интегрировать выбранную ML-библиотеку (Natasha, spaCy) для распознавания сущностей. Возможно, потребуется создать кастомные компоненты или дообучить модель на небольшом аннотированном наборе данных, если качество стандартных моделей будет недостаточным.

LLM-based NER: Написать функцию, которая отправляет чанки текста LLM с промптом для извлечения сущностей и их типов. Обработать ответ LLM.

Результат: Модуль, который на вход получает текст (чанк), а на выходе возвращает список найденных сущностей с их типами, текстовыми значениями и позициями в тексте.

Шаг 3.3: Реализация Модуля RE.

Что делать (выбрать один или комбинацию подходов):

Rule-based RE: Разработать правила (на основе ключевых слов, синтаксических шаблонов, близости сущностей в тексте) для определения отношений между сущностями, найденными модулем NER.

Пример: Если в одном предложении найдены СтатьяЗакона и УсловиеНазначенияПенсии, и между ними есть глагол "устанавливает", то создать отношение УСТАНАВЛИВАЕТ_УСЛОВИЕ.

LLM-based RE: Написать функцию, которая отправляет LLM текст (или пары сущностей с контекстом) с промптом для определения типа отношения между ними или извлечения полных триплетов.

Результат: Модуль, который на вход получает список сущностей (из NER) и текст, а на выходе возвращает список триплетов отношений (сущность1_id, тип_отношения, сущность2_id).

Шаг 3.4: Реализация Процесса Нормализации Сущностей.

Что делать:

Создать механизм для приведения извлеченных сущностей к каноническому виду. Например, "статья 8", "ст. 8", "восьмая статья" ФЗ-400 должны отображаться на один уникальный узел в графе, представляющий "Статья 8 ФЗ-400".

Это может включать словари синонимов, правила нормализации строк, возможно, использование ID из внешних справочников.

Результат: Функция, которая преобразует "сырые" извлеченные сущности в их нормализованные идентификаторы, используемые в графе.

Шаг 3.5: Сборка Пайплайна Извлечения.

Что делать: Написать основной скрипт, который:

Загружает документы с помощью loader.py.

Парсит их с помощью document_parser.py (возможно, адаптированного).

Для каждого чанка/документа:

Применяет NER.

Применяет RE.

Нормализует сущности.

Сохраняет извлеченные сущности (с атрибутами) и отношения (триплеты) в промежуточном формате (например, списки словарей, JSON-файлы) для последующей загрузки в граф.

Результат: Набор файлов с данными для графа.

Шаг 3.6: Оценка Качества Извлечения (пилотная).

Что делать:

Вручную разметить небольшой пилотный набор текстов (несколько статей закона) на предмет сущностей и отношений.

Запустить пайплайн извлечения на этих текстах.

Сравнить результаты автоматического извлечения с ручной разметкой. Оценить точность (precision) и полноту (recall).

Итеративно улучшать правила/промпты/модели на основе анализа ошибок.

Результат: Понимание качества извлечения и направления для улучшения.

Этап 4: Построение и Хранение Графа

Задача: Загрузить извлеченные данные в графовую СУБД и убедиться в корректности структуры графа.

Ключевые действия:

Шаг 4.1: Разработка Скриптов для Загрузки Данных в Граф.

Что делать:

Написать Python-скрипты, использующие драйвер выбранной графовой СУБД (например, neo4j-driver для Neo4j).

Скрипты должны:

Читать промежуточные файлы с извлеченными сущностями и отношениями.

Для каждой сущности: создавать узел в графе с соответствующим типом (меткой) и атрибутами. Обеспечить уникальность узлов (например, через MERGE в Cypher на основе ID сущности).

Для каждого отношения: создавать ребро в графе между соответствующими узлами с указанием типа отношения.

Результат: Скрипты для наполнения/обновления графа.

Шаг 4.2: Первичная Загрузка Данных в Граф.

Что делать: Запустить скрипты загрузки на данных, извлеченных из пилотного набора документов.

Результат: Граф знаний, содержащий информацию из пилотных документов.

Шаг 4.3: Тестирование Запросов к Графу и Валидация Структуры.

Что делать:

Написать и выполнить несколько тестовых запросов на языке графовой СУБД (например, Cypher).

Примеры запросов: "Найти все статьи Закона ФЗ-400", "Найти все условия назначения страховой пенсии по старости и статьи, которые их устанавливают", "Найти все документы, требуемые для пенсии по инвалидности".

Проверить, что граф построен корректно, узлы и ребра созданы в соответствии с онтологией, атрибуты заполнены.

Использовать инструменты визуализации графа (например, Neo4j Browser) для инспекции.

Результат: Уверенность в корректности построенного графа.

Шаг 4.4: Планирование Обновления Графа.

Что делать: Продумать, как граф будет обновляться при изменениях в законодательстве или улучшении моделей извлечения. Будет ли это полная перезаливка или инкрементальное обновление?

Результат: Стратегия поддержки графа в актуальном состоянии.

Фаза III: Интеграция Графа с RAG-Системой и Тестирование
Цель: Использовать граф знаний для улучшения процесса поиска и генерации ответов в существующей RAG-системе.

Этап 5: Интеграция Графа с RAG-Пайплайном (engine.py)

Задача: Модифицировать RAG-систему для использования графа знаний на этапе ретривинга и/или обогащения контекста.

Ключевые действия:

Шаг 5.1: Проектирование Способа Интеграции Графа.

Что делать: Выбрать один из способов интеграции (или их комбинацию):

А) Graph Retriever: Извлечение сущностей из запроса пользователя, формирование графового запроса, получение релевантных узлов/текстов из графа.

Б) Гибридный поиск: Параллельный векторный поиск и поиск по графу, с последующим объединением и переранжированием результатов.

В) Расширение контекста: Начальный векторный поиск, затем использование графа для нахождения связанных узлов и добавления их текстов в контекст.

Рекомендация для старта: Начать с варианта (А) или (В) как более простых для первой итерации.

Обсуждение/Решение: Какой способ интеграции будет реализован в первую очередь?

Шаг 5.2: Реализация Логики Извлечения Сущностей из Запроса Пользователя.

Что делать:

В методе query или _retrieve_nodes файла engine.py добавить логику для анализа текста запроса пользователя (case_description или query_bundle.query_str).

Использовать тот же NER-модуль (rule-based, ML или LLM-based), что и для наполнения графа, или упрощенную его версию, для идентификации ключевых сущностей (например, тип пенсии, категория граждан, условие).

Результат: Функция, возвращающая список сущностей, найденных в запросе.

Шаг 5.3: Реализация Формирования и Выполнения Графовых Запросов.

Что делать:

На основе сущностей, извлеченных из запроса пользователя, и выбранного способа интеграции, написать логику для динамического формирования запросов к графовой СУБД (например, строк Cypher).

Интегрировать Python-драйвер графовой СУБД для выполнения этих запросов.

Обработать результаты: извлечь ID статей, тексты, метаданные из узлов, полученных от графа.

Пример: Если из запроса извлечены "досрочная пенсия" (ТипПенсии) и "северяне" (КатегорияГраждан), сформировать Cypher-запрос, который ищет статьи, связанные с этими двумя сущностями.

Результат: Функция _retrieve_from_graph(query_bundle: QueryBundle) -> List[NodeWithScore] (или аналогичная), возвращающая узлы, найденные через граф.

Шаг 5.4: Модификация Метода _retrieve_nodes.

Что делать:

Интегрировать вызов новой функции _retrieve_from_graph в существующий метод _retrieve_nodes.

Реализовать логику объединения результатов векторного поиска и графового поиска (если используется гибридный подход). Это может включать дедупликацию узлов, присвоение весов результатам из разных источников.

Результат: Обновленный _retrieve_nodes, использующий граф.

Шаг 5.5: Адаптация Этапа Re-ranking.

Что делать: Убедиться, что узлы, полученные из графа (и их скоры, если есть), корректно обрабатываются существующим реранкером _rerank_nodes. Возможно, потребуется адаптировать логику скоринга или передачу метаданных.

Результат: Реранкер, работающий с объединенным набором кандидатов.

Шаг 5.6: Адаптация Этапа Построения Промпта (_build_prompt).

Что делать:

Модифицировать _build_prompt так, чтобы он мог использовать дополнительную структурированную информацию, полученную из графа.

Например, если граф показывает явную связь между двумя статьями, эту информацию можно включить в промпт, чтобы LLM лучше поняла контекст.

Метаданные из узлов графа (названия статей, типы условий) также могут быть использованы для более информативного контекста.

Результат: Улучшенный промпт, обогащенный знаниями из графа.

Этап 6: Тестирование и Оценка

Задача: Оценить влияние графа знаний на качество работы RAG-системы.

Ключевые действия:

Шаг 6.1: Разработка Сценариев Тестирования.

Что делать: Подготовить набор тестовых запросов, включая те, на которых текущая система испытывает трудности или где граф потенциально может дать значительное улучшение (например, запросы, требующие понимания связей между несколькими законами/статьями, или запросы с неявными условиями).

Результат: Набор тестовых кейсов.

Шаг 6.2: Проведение Сквозного Тестирования.

Что делать: Запустить тестовые запросы через обновленный RAG-пайплайн. Собрать ответы LLM.

Результат: Результаты работы системы с интегрированным графом.

Шаг 6.3: Сравнительный Анализ Результатов.

Что делать:

Сравнить ответы системы с графом и без графа (если возможно сохранить предыдущую версию или проводить A/B тестирование).

Оценить по критериям: релевантность найденных источников, точность и полнота ответа LLM, корректность ссылок на законодательство.

Можно использовать как качественные оценки (экспертный анализ ответов), так и количественные (если есть метрики, например, точность отнесения к категории "соответствует" / "не соответствует").

Результат: Выводы о влиянии графа знаний на производительность системы.

Фаза IV: Итерации и Развитие
Цель: Постоянное улучшение графа знаний и его интеграции.

Этап 7: Итеративное Улучшение

Задача: На основе результатов тестирования и обратной связи дорабатывать граф и RAG-систему.

Ключевые действия:

Шаг 7.1: Анализ Ошибок и Недостатков.

Что делать: Выявить случаи, где граф не помог или привел к ухудшению результатов. Проанализировать причины (неполная онтология, ошибки извлечения, неэффективные графовые запросы, проблемы интеграции).

Шаг 7.2: Расширение Онтологии.

Что делать: Добавлять новые типы сущностей и отношений по мере необходимости.

Шаг 7.3: Улучшение Моделей/Правил Извлечения.

Что делать: Дорабатывать NER/RE компоненты для повышения точности и полноты.

Шаг 7.4: Оптимизация Запросов к Графу.

Что делать: Пересматривать и оптимизировать графовые запросы для повышения производительности.

Шаг 7.5: Рассмотрение Более Продвинутых Техник.

Что делать: Исследовать возможность использования логического вывода на графе (inference), обучения графовых эмбеддингов для улучшения поиска, более сложных алгоритмов обхода графа.



## Детализированный План Доработки Системы Анализа Пенсионных Дел

### Часть 1: Бэкенд (Python)

#### Этап 1.1: Обновление Моделей Данных и Схемы БД

1.  **Добавление `record_number` в Pydantic модель `WorkBookRecordEntry`:**
    *   **Файл:** `app/models.py`
    *   **Действие:** В классе `WorkBookRecordEntry` добавить поле: `record_number: Optional[int] = Field(None, description="Порядковый номер записи из трудовой книжки")`.

2.  **Использование `DateTime(timezone=True)`:**
    *   **Файл:** `app/database.py`
    *   **Действие:** В определении `cases_table`, для колонок `created_at` и `updated_at` заменить `DateTime` на `DateTime(timezone=True)`.

3.  **Использование `sqlalchemy.JSON`:**
    *   **Файл:** `app/database.py`
    *   **Действие:** В определении `cases_table`, для колонок `personal_data`, `errors`, `disability`, `work_experience`, `benefits`, `documents`, `other_documents_extracted_data` заменить тип `Text` (или аналогичный строковый) на `JSON` (импортировать `from sqlalchemy import JSON`).

4.  **Добавление индексов в БД:**
    *   **Файл:** `app/database.py`
    *   **Действие:**
        *   Для `cases_table`: добавить `index=True` к колонкам `pension_type`, `final_status`, `created_at`.
        *   Для `ocr_tasks_table`: добавить `index=True` к колонкам `status`, `expire_at`.

#### Этап 1.2: Обновление Сервиса Обработки Изображений (OCR)

1.  **Обновление промпта для трудовой книжки:**
    *   **Файл:** `app/vision_services.py`
    *   **Действие:** В строке `WORK_BOOK_EXTRACTION_PROMPT_TEMPLATE`:
        *   Добавить инструкцию для LLM извлекать `record_number`.
        *   Обновить пример JSON в промпте, включив в него `record_number`.

2.  **Обновление Pydantic модели `WorkBookRecordEntry` (если используется для парсинга ответа LLM в `vision_services.py`):**
    *   **Файл:** `app/vision_services.py` (если модель определена локально) или `app/models.py` (если используется общая).
    *   **Действие:** Убедиться, что модель `WorkBookRecordEntry`, используемая для разбора ответа LLM, содержит поле `record_number: Optional[int]`.

#### Этап 1.3: Улучшение Аутентификации

1.  **Добавление `is_active` в JWT токен:**
    *   **Файл:** `app/auth.py`
    *   **Действие (в `login_for_access_token`):** При формировании `data` для `create_access_token`, добавить ` "is_active": user_in_db.is_active`.
    *   **Действие (в `get_current_user`):**
        *   Извлечь `is_active_from_token: Optional[bool] = payload.get("is_active")`.
        *   Если `is_active_from_token is False`, генерировать `HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="User is inactive based on token")`.
        *   Обновить возвращаемый словарь, чтобы он включал ` "is_active": is_active_from_token if is_active_from_token is not None else True` (или более строгую логику, если `is_active` обязательно должно быть в новых токенах).
    *   **Действие (в `require_role_dependency_factory` или аналогичной функции):** Добавить проверку `if not current_user.get("is_active"): raise HTTPException(...)` в начало функции `role_checker` (или аналогичной).

2.  **Безопасность `SECRET_KEY`:**
    *   **Файл:** `app/auth.py`
    *   **Действие:** Проверить, что `SECRET_KEY = os.getenv("SECRET_KEY", "...")` используется, и напомнить (в комментарии или документации) о необходимости установки переменной окружения `SECRET_KEY` в продакшене.

#### Этап 1.4: Доработка RAG Core

1.  **Изменение ключа кэша RAG:**
    *   **Файл:** `app/main.py` (внутри фоновой задачи `process_case_in_background`)
    *   **Действие:**
        *   Импортировать `hashlib` и `json`.
        *   Изменить формирование `cache_key`:
            ```python
            sorted_case_data_json = json.dumps(case_data_dict, sort_keys=True) # case_data_dict уже словарь
            cache_key_hash = hashlib.md5(sorted_case_data_json.encode('utf-8')).hexdigest()
            cache_key = f"rag_case_{case_data_dict.get('pension_type')}_{cache_key_hash}"
            ```

2.  **Передача `temperature` для LLM как параметра:**
    *   **Файл:** `app/rag_core/engine.py` (в методе `PensionRAG.query`)
    *   **Действие:**
        *   Попытаться передать `temperature=0.1` напрямую в метод `await self.llm.acomplete(final_prompt, temperature=0.1)`. Проверить, поддерживает ли это используемая версия LlamaIndex и клиент Ollama.
        *   Если параметр не поддерживается, оставить текущую логику с сохранением и восстановлением `self.llm.temperature`, но добавить `import asyncio` и обернуть блок изменения `temperature` и вызова LLM в `async with self.llm_temp_lock:` (где `self.llm_temp_lock = asyncio.Lock()` инициализируется в `__init__` `PensionRAG`). Это предотвратит гонки состояний, если экземпляр `self.llm` общий.

#### Этап 1.5: Управление Транзакциями в CRUD

1.  **Проверка управления транзакциями:**
    *   **Файл:** `app/crud.py` и файлы, вызывающие CRUD-функции (например, `app/main.py`).
    *   **Действие:** Проанализировать функции в `crud.py`. Если функция (например, `create_case`, `update_ocr_task_result`) вызывается внутри блока `async with conn.begin():` в `main.py` или сервисном слое, то явный `await conn.commit()` внутри CRUD-функции избыточен и его можно удалить. Если CRUD-функция задумана как полностью атомарная операция, которая может вызываться и без внешнего управления транзакцией, то внутренний коммит оставить. Цель – избежать вложенных транзакций без необходимости или двойных коммитов.

---

### Часть 2: Фронтенд (React, TypeScript)

#### Этап 2.1: Обновление Типов Данных

1.  **Добавление `record_number` в `WorkBookRecordEntry`:**
    *   **Файл:** `src/types.ts`
    *   **Действие:** В интерфейсе `WorkBookRecordEntry` добавить поле `record_number: number | null;`.

2.  **Использование Enum для строковых литералов (пример для `gender`):**
    *   **Файл:** `src/types.ts`
    *   **Действие:**
        *   Создать `export enum Gender { Male = 'male', Female = 'female' }`.
        *   В интерфейсе `PersonalData`, изменить тип поля `gender` на `gender: Gender | '';` (или `Gender | null`, в зависимости от того, как обрабатываются пустые значения).
    *   **Действие (в компонентах):** Обновить использование поля `gender` (например, в `PersonalDataStep.tsx` для `Select` использовать `Gender.Male` и `Gender.Female` в качестве `value`). Аналогично для других полей, где это применимо (`disability.group`, `OcrTaskStatus`).

3.  **Согласованность имен полей для документов:**
    *   **Файлы:** `src/types.ts` (для `CaseFormDataTypeForRHF`), `src/utils.ts` (в `prepareDataForApi`), `src/components/formSteps/AdditionalInfoStep.tsx`.
    *   **Действие:** Решить, будет ли поле для стандартных документов в RHF называться `documents` или `submitted_documents`.
        *   **Если `submitted_documents` (рекомендуется для ясности):**
            *   В `CaseFormDataTypeForRHF`: переименовать `documents?: string;` в `submitted_documents?: string;`.
            *   В `AdditionalInfoStep.tsx`: изменить `name="documents"` и `control={control}` для `Controller` на `name="submitted_documents"`.
            *   В `prepareDataForApi`: изменить `documents: (formData.documents || '')...` на `submitted_documents: (formData.submitted_documents || '')...`.
        *   **Если оставить `documents` в RHF:** Убедиться, что `prepareDataForApi` корректно маппит `formData.documents` в `apiPayload.submitted_documents`. Текущая версия `prepareDataForApi` из вашего файла, кажется, ожидает `formData.documents` и маппит его в `dataToSend.documents`, что не соответствует `CaseDataInput.submitted_documents`. **Это нужно исправить.**

#### Этап 2.2: Реализация Многостраничной Загрузки Трудовой Книжки

1.  **Создание/Модификация `OcrUploader` для многостраничности (`MultiPageOcrUploader.tsx`):**
    *   **Файл:** Создать новый `src/components/formInputs/MultiPageOcrUploader.tsx` или значительно модифицировать существующий `OcrUploader.tsx`.
    *   **Действие:**
        *   В `UploadProps` установить `multiple: true`.
        *   Управлять состоянием `fileList` для накопления выбранных файлов.
        *   Реализовать `beforeUpload` так, чтобы файлы добавлялись в `fileList`, а не загружались автоматически (`return false;`).
        *   Добавить кнопку "Обработать все загруженные страницы".
        *   Реализовать функцию `handleProcessAll` (или аналогичную), которая по нажатию на кнопку:
            *   Последовательно отправляет каждый файл из `fileList` на OCR-эндпоинт (`submitOcrTask`).
            *   Для каждого файла запускает поллинг статуса (`pollOcrStatus`).
            *   Вызывает коллбэк `onOcrSuccessPerPage(data, docType, fileName, pageIndex, totalPages)` при успешной обработке каждой страницы.
            *   Обрабатывает ошибки для каждой страницы, вызывает `onOcrError`.
            *   Управляет общим состоянием загрузки (`isProcessingAll`).
        *   Опционально: реализовать UI для изменения порядка файлов в `fileList`.

2.  **Интеграция `MultiPageOcrUploader` в `DocumentUploadStep.tsx`:**
    *   **Файл:** `src/components/formSteps/DocumentUploadStep.tsx`
    *   **Действие:**
        *   Использовать `MultiPageOcrUploader` для типа документа `work_book`.
        *   Создать состояние `workBookPagesData: WorkBookRecordEntry[]` для сбора записей со всех страниц.
        *   Реализовать `handleWorkBookPageSuccess`: добавлять полученные `WorkBookRecordEntry` в `workBookPagesData`.
        *   Добавить состояние `allWorkBookPagesProcessed: boolean`, которое устанавливается в `true`, когда все отправленные страницы трудовой обработаны.
        *   В `useEffect`, который зависит от `allWorkBookPagesProcessed` и `workBookPagesData`:
            *   Сортировать `workBookPagesData` по `record_number` (первичный ключ) и `date_in` (вторичный).
            *   Выполнить опциональную проверку на корректность нумерации записей.
            *   Установить отсортированные и смапленные записи в RHF: `setValue('work_experience.records', mappedSortedRecords)`.
            *   Если решено рассчитывать общий стаж на клиенте, рассчитать его здесь и установить: `setValue('work_experience.total_years', calculatedTotalYears)`.
            *   Вызвать `trigger` для обновленных полей.

3.  **Расчет/Ввод Общего Стажа:**
    *   **Файлы:** `src/components/formSteps/WorkExperienceStep.tsx`, `src/components/formSteps/DocumentUploadStep.tsx`.
    *   **Действие:** Принять решение:
        *   **Оставить ручной ввод:** В `WorkExperienceStep.tsx` поле "Общий подтвержденный стаж" остается редактируемым.
        *   **Клиентский расчет:** Реализовать функцию `calculateClientSideTotalYears(records: WorkBookRecordEntry[]): number` (с учетом пересечений периодов, если это требуется). Вызывать ее в `DocumentUploadStep.tsx` после получения и сортировки всех записей и обновлять поле `work_experience.total_years` в RHF. Поле в `WorkExperienceStep.tsx` можно сделать `readOnly` или отображать расчетное значение рядом с полем для ручного ввода.

#### Этап 2.3: Улучшения UI/UX и Вспомогательных Функций

1.  **Консистентность работы с датами:**
    *   **Файлы:** `src/utils.ts`, `src/components/formSteps/PersonalDataStep.tsx`, `src/components/formSteps/WorkExperienceStep.tsx`, `src/components/formSteps/DisabilityInfoStep.tsx`, `src/pages/CaseHistoryPage.tsx`, `src/pages/CaseDetailPage.tsx`.
    *   **Действие:** Выбрать одну библиотеку (`dayjs` или `date-fns`). Если `dayjs` уже используется в большинстве мест, заменить использование `date-fns` в `src/utils.ts` на `dayjs`. Обеспечить корректное форматирование (`YYYY-MM-DD` для API, `DD.MM.YYYY` для отображения) и парсинг дат во всех компонентах.

2.  **Корректность `prepareDataForApi`:**
    *   **Файл:** `src/utils.ts`
    *   **Действие:** Убедиться, что поле `submitted_documents` для API формируется из правильного поля RHF (например, `formData.documents` или `formData.submitted_documents`). Сейчас в `prepareDataForApi` используется `formData.documents` и маппится в `dataToSend.documents`. Это нужно исправить на `dataToSend.submitted_documents` в соответствии с `CaseDataInput`.

3.  **Локализация дат:**
    *   **Файлы:** Везде, где отображаются даты (например, `CaseHistoryPage.tsx`, `CaseDetailPage.tsx`).
    *   **Действие:** Если используется `dayjs`, убедиться, что `dayjs.locale('ru')` вызван (как в `SystemHealthPage.tsx`) и используется при форматировании дат для отображения (например, `dayjs(date).format('DD MMMM YYYY HH:mm')`).

4.  **Обратная связь при загрузке многостраничных документов:**
    *   **Файл:** `src/components/formInputs/MultiPageOcrUploader.tsx` (или модифицированный `OcrUploader.tsx`).
    *   **Действие:** Улучшить `antdMessage` или добавить другую индикацию, показывающую прогресс обработки страниц (например, "Обработано N из M страниц...").

5.  **Инструкции для пользователя по порядку страниц:**
    *   **Файл:** `src/components/formSteps/DocumentUploadStep.tsx` (в части для `work_book`).
    *   **Действие:** Добавить текстовое пояснение для пользователя о важности загрузки страниц трудовой книжки в правильном хронологическом порядке.
